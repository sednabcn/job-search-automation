name: Load and Validate Existing Data

on:
  workflow_dispatch:
    inputs:
      validate_only:
        description: 'Only validate data without processing'
        required: false
        type: boolean
        default: false
  
  push:
    branches:
      - main
    paths:
      - 'job_search/*.json'

jobs:
  load_and_validate:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Load and validate existing JSON files
        run: |
          python3 << 'PYTHON_SCRIPT'
          import json
          import os
          from datetime import datetime
          from pathlib import Path
          
          print("üìÇ Loading existing job search data...\n")
          
          job_search_dir = Path('job_search')
          
          if not job_search_dir.exists():
              print("‚ö†Ô∏è  job_search directory does not exist!")
              print("Creating job_search directory...")
              job_search_dir.mkdir(exist_ok=True)
          
          # Define expected files and their structure
          data_files = {
              'applications.json': {
                  'required_fields': ['company', 'position', 'status'],
                  'optional_fields': ['applied_date', 'url', 'source', 'notes']
              },
              'companies.json': {
                  'required_fields': ['company_name'],
                  'optional_fields': ['overall_rating', 'industry', 'size', 'headquarters']
              },
              'interviews.json': {
                  'required_fields': ['company_name', 'position'],
                  'optional_fields': ['interview_date', 'difficulty', 'experience']
              },
              'salaries.json': {
                  'required_fields': ['company_name', 'role'],
                  'optional_fields': ['base_salary_avg', 'total_comp_avg', 'years_experience']
              },
              'reviews.json': {
                  'required_fields': ['company_name'],
                  'optional_fields': ['rating', 'review_text', 'date']
              }
          }
          
          stats = {
              'total_files': 0,
              'total_records': 0,
              'errors': [],
              'warnings': [],
              'files_processed': {}
          }
          
          # Process each expected file
          for filename, schema in data_files.items():
              file_path = job_search_dir / filename
              
              if not file_path.exists():
                  print(f"üìÑ {filename}: File not found - will be created when data is added")
                  stats['files_processed'][filename] = 'missing'
                  continue
              
              try:
                  with open(file_path, 'r') as f:
                      content = f.read().strip()
                      
                      if not content:
                          print(f"üìÑ {filename}: Empty file")
                          stats['files_processed'][filename] = 'empty'
                          continue
                      
                      data = json.loads(content)
                      
                      if not isinstance(data, list):
                          stats['errors'].append(f"{filename}: Data should be a JSON array, got {type(data).__name__}")
                          print(f"‚ùå {filename}: Invalid format (should be array)")
                          continue
                      
                      record_count = len(data)
                      stats['total_files'] += 1
                      stats['total_records'] += record_count
                      stats['files_processed'][filename] = 'valid'
                      
                      print(f"‚úÖ {filename}: {record_count} records")
                      
                      # Validate each record
                      for i, record in enumerate(data):
                          if not isinstance(record, dict):
                              stats['warnings'].append(f"{filename}[{i}]: Record is not an object")
                              continue
                          
                          # Check required fields
                          missing = [f for f in schema['required_fields'] if f not in record]
                          if missing:
                              stats['warnings'].append(
                                  f"{filename}[{i}]: Missing required fields: {', '.join(missing)}"
                              )
                      
                      # Show sample of first record
                      if data:
                          sample_keys = list(data[0].keys())[:5]
                          print(f"   Sample keys: {', '.join(sample_keys)}")
              
              except json.JSONDecodeError as e:
                  stats['errors'].append(f"{filename}: JSON parse error - {str(e)}")
                  print(f"‚ùå {filename}: Invalid JSON - {str(e)}")
                  stats['files_processed'][filename] = 'invalid_json'
              
              except Exception as e:
                  stats['errors'].append(f"{filename}: Unexpected error - {str(e)}")
                  print(f"‚ùå {filename}: Error - {str(e)}")
                  stats['files_processed'][filename] = 'error'
          
          # Generate summary report
          print("\n" + "="*60)
          print("üìä SUMMARY REPORT")
          print("="*60)
          print(f"Files processed: {stats['total_files']}")
          print(f"Total records: {stats['total_records']}")
          print(f"Errors: {len(stats['errors'])}")
          print(f"Warnings: {len(stats['warnings'])}")
          
          # File status summary
          print(f"\nüìÅ File Status:")
          for filename, status in stats['files_processed'].items():
              status_emoji = {
                  'valid': '‚úÖ',
                  'empty': 'üì≠',
                  'missing': '‚ùå',
                  'invalid_json': '‚ö†Ô∏è',
                  'error': '‚ùå'
              }.get(status, '‚ùì')
              print(f"  {status_emoji} {filename}: {status}")
          
          # Show errors
          if stats['errors']:
              print(f"\n‚ùå ERRORS ({len(stats['errors'])}):")
              for error in stats['errors']:
                  print(f"  ‚Ä¢ {error}")
          
          # Show warnings
          if stats['warnings']:
              print(f"\n‚ö†Ô∏è  WARNINGS ({len(stats['warnings'])}):")
              for i, warning in enumerate(stats['warnings'][:10], 1):
                  print(f"  ‚Ä¢ {warning}")
              if len(stats['warnings']) > 10:
                  print(f"  ... and {len(stats['warnings']) - 10} more warnings")
          
          # Write report to file
          with open('data_validation_report.txt', 'w') as f:
              f.write('Job Search Data Validation Report\n')
              f.write(f'Generated: {datetime.now().isoformat()}\n')
              f.write('='*60 + '\n\n')
              f.write(f'Files processed: {stats["total_files"]}\n')
              f.write(f'Total records: {stats["total_records"]}\n')
              f.write(f'Errors: {len(stats["errors"])}\n')
              f.write(f'Warnings: {len(stats["warnings"])}\n\n')
              
              f.write('File Status:\n')
              for filename, status in stats['files_processed'].items():
                  f.write(f'  {filename}: {status}\n')
              
              if stats['errors']:
                  f.write(f'\nErrors:\n')
                  for error in stats['errors']:
                      f.write(f'  ‚Ä¢ {error}\n')
              
              if stats['warnings']:
                  f.write(f'\nWarnings:\n')
                  for warning in stats['warnings']:
                      f.write(f'  ‚Ä¢ {warning}\n')
          
          print(f"\n‚úÖ Validation complete!")
          print(f"üìÑ Full report saved to: data_validation_report.txt")
          
          # Exit with error code if there are errors
          if stats['errors']:
              print("\n‚ö†Ô∏è  Validation found errors - please review")
              exit(1)
          
          PYTHON_SCRIPT
      
      - name: Upload validation report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: data-validation-report-${{ github.run_number }}
          path: data_validation_report.txt
          retention-days: 30
      
      - name: Initialize automation if validation passed
        if: success() && github.event.inputs.validate_only != 'true'
        run: |
          python3 << 'PYTHON_SCRIPT'
          import sys
          import os
          
          # Add .github/scripts to Python path
          sys.path.insert(0, '.github/scripts')
          
          print("üîÑ Attempting to initialize GlassdoorAutomation with loaded data...")
          
          try:
              from glassdoor_automation import GlassdoorAutomation
              
              # Initialize - this should load data from job_search/*.json files
              gd = GlassdoorAutomation()
              
              print(f"‚úÖ Successfully initialized!")
              print(f"üìä Loaded data:")
              print(f"   Companies: {len(gd.companies)}")
              print(f"   Salaries: {len(gd.salaries)}")
              print(f"   Interviews: {len(gd.interviews)}")
              print(f"   Reviews: {len(gd.reviews)}")
              
              if hasattr(gd, 'applications'):
                  print(f"   Applications: {len(gd.applications)}")
              
          except ImportError as e:
              print(f"‚ö†Ô∏è  Could not import GlassdoorAutomation: {e}")
              print("This is OK - validation was successful")
          except Exception as e:
              print(f"‚ö†Ô∏è  Could not initialize automation: {e}")
              print("This is OK - validation was successful")
          
          PYTHON_SCRIPT
      
      - name: Create validation report issue
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let report = 'Validation report not available';
            if (fs.existsSync('data_validation_report.txt')) {
              report = fs.readFileSync('data_validation_report.txt', 'utf8');
            }
            
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: '‚ö†Ô∏è Job Search Data Validation Failed',
              body: `## Data Validation Errors\n\n\`\`\`\n${report}\n\`\`\`\n\nPlease fix the errors in your JSON files and try again.\n\n**Run:** [#${context.runNumber}](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})`,
              labels: ['data-validation', 'error']
            });
