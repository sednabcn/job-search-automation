name: Job Discovery Worker 
# Enhanced orchestrator supporting LinkedIn, Glassdoor, Indeed, and Reed

on:
  schedule:
    # Run full pipeline twice daily (morning & evening)
    - cron: '0 8,20 * * *'

  workflow_run:
    workflows:
      - "Job Search Manager Center - Multi-Platform"
    types:
      - completed
    branches:
      - master
  
  workflow_dispatch:
    inputs:
      mode:
        description: 'Execution Mode'
        required: true
        default: 'full'
        type: choice
        options:
          - full           # Complete pipeline
          - discovery      # Only job discovery
          - matching       # Only job matching
          - networking     # Only networking activities
          - reporting      # Only generate reports
          - maintenance    # Cleanup and optimization
          - keyword_extraction  # Extract keywords and update config
      
      job_boards:
        description: 'Job Boards to Scan'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - linkedin
          - glassdoor
          - indeed
          - reed
          - traditional    # LinkedIn + Glassdoor
          - modern         # Indeed + Reed
      
      auto_apply:
        description: 'Auto-apply to top matches'
        required: false
        default: 'false'
        type: boolean
      
      match_threshold:
        description: 'Minimum match score (0-100)'
        required: false
        default: '75'
        type: string
      
      update_keywords:
        description: 'Auto-update config with extracted keywords'
        required: false
        default: 'false'
        type: boolean

env:
  PYTHON_VERSION: '3.10'
  TIMEZONE: 'Europe/London'
  MAX_DAILY_APPLICATIONS: 50
  REPORTS_DIR: './reports'
  LOGS_DIR: './logs'
  SCRIPTS_DIR: '.github/scripts'

jobs:
  # ========================================
  # STAGE 1: INITIALIZATION & HEALTH CHECK
  # ========================================
  initialize:
    name: System Initialization
    runs-on: ubuntu-latest
    outputs:
      run_id: ${{ steps.setup.outputs.run_id }}
      timestamp: ${{ steps.setup.outputs.timestamp }}
      mode: ${{ steps.setup.outputs.mode }}
      platforms: ${{ steps.setup.outputs.platforms }}
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Setup Python Environment
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install Dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install beautifulsoup4 requests pyyaml
          
          # Add scripts directory to Python path
          echo "PYTHONPATH=$GITHUB_WORKSPACE/${{ env.SCRIPTS_DIR }}:$PYTHONPATH" >> $GITHUB_ENV
      
      - name: Initialize Run Context
        id: setup
        run: |
          RUN_ID="job-search-$(date +%Y%m%d-%H%M%S)"
          TIMESTAMP=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          MODE="${{ github.event.inputs.mode || 'full' }}"
          
          # Determine which platforms to use
          JOB_BOARDS="${{ github.event.inputs.job_boards || 'all' }}"
          if [ "$JOB_BOARDS" = "all" ]; then
            PLATFORMS="linkedin,glassdoor,indeed,reed"
          elif [ "$JOB_BOARDS" = "traditional" ]; then
            PLATFORMS="linkedin,glassdoor"
          elif [ "$JOB_BOARDS" = "modern" ]; then
            PLATFORMS="indeed,reed"
          else
            PLATFORMS="$JOB_BOARDS"
          fi
          
          echo "run_id=$RUN_ID" >> $GITHUB_OUTPUT
          echo "timestamp=$TIMESTAMP" >> $GITHUB_OUTPUT
          echo "mode=$MODE" >> $GITHUB_OUTPUT
          echo "platforms=$PLATFORMS" >> $GITHUB_OUTPUT
          
          echo "üöÄ Starting Job Search Manager Center"
          echo "Run ID: $RUN_ID"
          echo "Mode: $MODE"
          echo "Platforms: $PLATFORMS"
          echo "Timestamp: $TIMESTAMP"
      
      - name: Health Check - Verify Data Files
        run: |
          python3 << 'EOF'
          import json
          import os
          from pathlib import Path
          
          required_files = [
              'applications.json',
              'discovered_jobs.json',
              'network_contacts.json',
              'resume_versions.json',
              'saved_searches.json',
              'application_deadlines.json',
              'indeed_jobs.json',
              'reed_jobs.json'
          ]
          
          for file in required_files:
              filepath = Path(file)
              if not filepath.exists():
                  print(f"‚ö†Ô∏è  Creating missing file: {file}")
                  with open(filepath, 'w') as f:
                      json.dump([], f)
              else:
                  print(f"‚úÖ Found: {file}")
          
          # Verify JSON integrity
          for file in required_files:
              try:
                  with open(file, 'r') as f:
                      json.load(f)
                  print(f"‚úÖ Valid JSON: {file}")
              except json.JSONDecodeError:
                  print(f"‚ùå Corrupted JSON: {file}")
                  exit(1)
          EOF
      
      - name: Create Directory Structure
        run: |
          mkdir -p ${{ env.LOGS_DIR }}/$(date +%Y-%m-%d)
          mkdir -p ${{ env.REPORTS_DIR }}/$(date +%Y-%m-%d)
          mkdir -p scrapers
          mkdir -p configs

  # ========================================
  # STAGE 2: JOB DISCOVERY - MULTI-PLATFORM
  # ========================================
  discover-jobs:
    name: Job Discovery - ${{ matrix.platform }}
    runs-on: ubuntu-latest
    needs: initialize
    if: |
      needs.initialize.outputs.mode == 'full' || 
      needs.initialize.outputs.mode == 'discovery'
    
    strategy:
      matrix:
        platform: [linkedin, glassdoor, indeed, reed]
      fail-fast: false
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Check if Platform Enabled
        id: check_platform
        run: |
          PLATFORMS="${{ needs.initialize.outputs.platforms }}"
          if [[ "$PLATFORMS" == *"${{ matrix.platform }}"* ]]; then
            echo "enabled=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Platform ${{ matrix.platform }} is enabled"
          else
            echo "enabled=false" >> $GITHUB_OUTPUT
            echo "‚è≠Ô∏è  Skipping ${{ matrix.platform }}"
          fi
      
      - name: Setup Python
        if: steps.check_platform.outputs.enabled == 'true'
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install Dependencies
        if: steps.check_platform.outputs.enabled == 'true'
        run: |
          pip install -r requirements.txt
          pip install beautifulsoup4 requests lxml
          
          # Add scripts directory to Python path
          echo "PYTHONPATH=$GITHUB_WORKSPACE/${{ env.SCRIPTS_DIR }}:$PYTHONPATH" >> $GITHUB_ENV

      - name: Run LinkedIn Discovery
        if: steps.check_platform.outputs.enabled == 'true' && matrix.platform == 'linkedin'
        run: |
          echo "üîç Discovering jobs on LinkedIn..."
          echo "‚ö†Ô∏è  Note: LinkedIn requires manual authentication"
          
          # Check if a LinkedIn scraper exists
          if [ -f "${{ env.SCRIPTS_DIR }}/linkedin_job_scraper.py" ]; then
            python ${{ env.SCRIPTS_DIR }}/linkedin_job_scraper.py \
              --mode discover \
              --output linkedin_jobs.json
          else
            # Create empty placeholder
            echo "Creating placeholder file..."
            echo '[]' > linkedin_jobs.json
            echo "üí° Add LinkedIn jobs manually to linkedin_jobs.json"
          fi
        env:
          LINKEDIN_EMAIL: ${{ secrets.LINKEDIN_EMAIL }}
          LINKEDIN_PASSWORD: ${{ secrets.LINKEDIN_PASSWORD }}
      
      - name: Run Glassdoor Discovery
        if: steps.check_platform.outputs.enabled == 'true' && matrix.platform == 'glassdoor'
        run: |
          echo "üîç Discovering jobs on Glassdoor..."
          python ${{ env.SCRIPTS_DIR }}/enhanced_glassdoor_automotion.py --scrape --output glassdoor_jobs.json
        env:
          GLASSDOOR_EMAIL: ${{ secrets.GLASSDOOR_EMAIL }}
          GLASSDOOR_PASSWORD: ${{ secrets.GLASSDOOR_PASSWORD }}
      
      - name: Run Indeed Discovery
        if: steps.check_platform.outputs.enabled == 'true' && matrix.platform == 'indeed'
        run: |
          echo "üîç Discovering jobs on Indeed..."
          python ${{ env.SCRIPTS_DIR }}/indeed_scraper.py --search --output indeed_jobs.json
        env:
          INDEED_MAX_RESULTS: 100

      - name: Run Reed Discovery
        if: steps.check_platform.outputs.enabled == 'true' && matrix.platform == 'reed'
        run: |
          echo "üîç Discovering jobs on Reed..."
          
          # Load search keywords from config
          KEYWORDS=$(python3 -c "
          import yaml
          try:
              with open('job-manager-config.yml', 'r') as f:
                  config = yaml.safe_load(f)
              roles = config.get('search', {}).get('target_roles', [])
              print(roles[0] if roles else 'Software Engineer')
          except:
              print('Software Engineer')
          ")
          
          if [ -n "${{ secrets.REED_API_KEY }}" ]; then
            python ${{ env.SCRIPTS_DIR }}/reed_scraper.py \
              --keywords "$KEYWORDS" \
              --location "London" \
              --max-results ${{ env.REED_MAX_RESULTS || '100' }} \
              --api-key "${{ secrets.REED_API_KEY }}" \
              --use-api \
              --output reed_jobs.json
          else
            python ${{ env.SCRIPTS_DIR }}/reed_scraper.py \
              --keywords "$KEYWORDS" \
              --location "London" \
              --max-results ${{ env.REED_MAX_RESULTS || '100' }} \
              --output reed_jobs.json
          fi
        env:
          REED_API_KEY: ${{ secrets.REED_API_KEY }}
      
      - name: Enhance Job Data
        if: steps.check_platform.outputs.enabled == 'true'
        run: |
          python3 << 'EOF'
          import json
          from datetime import datetime
          from pathlib import Path
          
          platform = "${{ matrix.platform }}"
          filename = f"{platform}_jobs.json"
          
          if Path(filename).exists():
              with open(filename, 'r') as f:
                  jobs = json.load(f)
              
              # Add metadata
              for job in jobs:
                  job['discovery_date'] = datetime.now().isoformat()
                  job['discovery_run_id'] = "${{ needs.initialize.outputs.run_id }}"
                  job['platform'] = platform
              
              with open(filename, 'w') as f:
                  json.dump(jobs, f, indent=2)
              
              print(f"‚úÖ Enhanced {len(jobs)} jobs from {platform}")
          EOF
      
      - name: Upload Discovered Jobs
        if: steps.check_platform.outputs.enabled == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: discovered-jobs-${{ matrix.platform }}
          path: ${{ matrix.platform }}_jobs.json
          retention-days: 7
      
      - name: Log Discovery Stats
        if: steps.check_platform.outputs.enabled == 'true'
        run: |
          python3 << 'EOF'
          import json
          from pathlib import Path
          
          platform = "${{ matrix.platform }}"
          filename = f"{platform}_jobs.json"
          
          if Path(filename).exists():
              with open(filename, 'r') as f:
                  jobs = json.load(f)
              
              print(f"üìä {platform.title()} Discovery Stats:")
              print(f"   Total jobs found: {len(jobs)}")
              print(f"   Unique companies: {len(set(j.get('company', '') for j in jobs))}")
              print(f"   Locations: {len(set(j.get('location', '') for j in jobs))}")
          EOF

  # ========================================
  # STAGE 2.5: KEYWORD EXTRACTION (Optional)
  # ========================================
  extract-keywords:
    name: Extract Keywords & Update Config
    runs-on: ubuntu-latest
    needs: [initialize, discover-jobs]
    if: |
      always() &&
      (needs.initialize.outputs.mode == 'keyword_extraction' ||
       github.event.inputs.update_keywords == 'true')
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install Dependencies
        run: |
          pip install -r requirements.txt
          pip install pyyaml beautifulsoup4
          
          # Add scripts directory to Python path
          echo "PYTHONPATH=$GITHUB_WORKSPACE/${{ env.SCRIPTS_DIR }}:$PYTHONPATH" >> $GITHUB_ENV
      
      - name: Download All Discovered Jobs
        uses: actions/download-artifact@v4
        with:
          pattern: discovered-jobs-*
          merge-multiple: true
      
      - name: Run Keyword Extraction
        run: |
          echo "üîç Extracting keywords from job listings..."
          
          # Load target positions from config
          python3 << 'EOF'
          import yaml
          import subprocess
          
          try:
              with open('job-manager-config.yml', 'r') as f:
                  config = yaml.safe_load(f)
              positions = config.get('search', {}).get('target_roles', [])
          except:
              positions = ['Senior Software Engineer', 'Staff Engineer', 'Tech Lead']
          
          # Run keyword extractor
          positions_str = ' '.join([f'"{p}"' for p in positions])
          cmd = f'python ${{ env.SCRIPTS_DIR }}/keyword_extractor.py --files "*_jobs.json" --positions {positions_str} --output job-manager-config-updated.yml'
          subprocess.run(cmd, shell=True)
          EOF
      
      - name: Compare Configurations
        run: |
          echo "üìä Configuration Changes:"
          if [ -f job-manager-config-updated.yml ]; then
            diff -u job-manager-config.yml job-manager-config-updated.yml || true
          fi
      
      - name: Backup Old Config
        run: |
          if [ -f job-manager-config.yml ]; then
            mkdir -p configs
            cp job-manager-config.yml "configs/job-manager-config-backup-$(date +%Y%m%d-%H%M%S).yml"
          fi
      
      - name: Update Configuration
        run: |
          if [ -f job-manager-config-updated.yml ]; then
            mv job-manager-config-updated.yml job-manager-config.yml
            echo "‚úÖ Configuration updated"
          fi
      
      - name: Commit Updated Config
        run: |
          git config user.name "Job Search Bot"
          git config user.email "bot@jobsearch.local"
          git add job-manager-config.yml configs/
          git commit -m "ü§ñ Auto-updated config with extracted keywords - Run ${{ needs.initialize.outputs.run_id }}" || echo "No changes"
          git push || echo "Push failed"
      
      - name: Upload Analysis Report
        uses: actions/upload-artifact@v4
        with:
          name: keyword-analysis
          path: |
            keyword_analysis_report.json
            job-manager-config.yml
          retention-days: 30

  # ========================================
  # STAGE 3: JOB MATCHING & SCORING
  # ========================================
  match-and-score:
    name: Job Matching Engine
    runs-on: ubuntu-latest
    needs: [initialize, discover-jobs]
    if: |
      always() &&
      (needs.discover-jobs.result == 'success' || needs.discover-jobs.result == 'skipped') &&
      (needs.initialize.outputs.mode == 'full' || 
       needs.initialize.outputs.mode == 'matching')
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install Dependencies
        run: |
          pip install -r requirements.txt
          
          # Add scripts directory to Python path
          echo "PYTHONPATH=$GITHUB_WORKSPACE/${{ env.SCRIPTS_DIR }}:$PYTHONPATH" >> $GITHUB_ENV
      
      - name: Download Discovered Jobs
        uses: actions/download-artifact@v4
        with:
          pattern: discovered-jobs-*
          merge-multiple: true
      
      - name: Merge Multi-Platform Jobs
        run: |
          python3 << 'EOF'
          import json
          from pathlib import Path
          from datetime import datetime
          
          all_jobs = []
          seen_ids = set()
          platform_stats = {}
          
          # Merge all discovered job files
          for job_file in Path('.').glob('*_jobs.json'):
              try:
                  with open(job_file, 'r') as f:
                      jobs = json.load(f)
                  
                  platform = job_file.stem.replace('_jobs', '')
                  platform_stats[platform] = len(jobs)
                  
                  for job in jobs:
                      # Create unique ID
                      job_id = (job.get('id') or 
                               f"{job.get('company', 'unknown')}_{job.get('title', 'unknown')}")
                      
                      if job_id not in seen_ids:
                          all_jobs.append(job)
                          seen_ids.add(job_id)
              except Exception as e:
                  print(f"Error processing {job_file}: {e}")
          
          with open('discovered_jobs.json', 'w') as f:
              json.dump(all_jobs, f, indent=2)
          
          print(f"‚úÖ Merged {len(all_jobs)} unique jobs from {len(platform_stats)} platforms")
          for platform, count in platform_stats.items():
              print(f"   {platform}: {count} jobs")
          EOF

      - name: Run Job Matcher
        run: |
          echo "üéØ Running multi-platform job matcher..."
    
          # Create reports directory
          mkdir -p reports
    
          # Run analysis using Python script directly
          python3 << 'PYTHON_SCRIPT'
          import sys, os
          from pathlib import Path
          from datetime import datetime
    
          # Add script directory to path
          script_dir = os.path.join(os.getcwd(), '.github', 'scripts')
          sys.path.insert(0, script_dir)
          sys.path.insert(0, '.')
    
          print(f"Python path: {sys.path[:3]}")
          print(f"Current directory: {os.getcwd()}")
          print(f"Script directory: {script_dir}")
    
          try:
            from cv_job_matcher import CVJobMatcher
            print("‚úÖ Successfully imported CVJobMatcher")
          except ImportError as e:
            print(f"‚ùå Failed to import cv_job_matcher.py")
            print(f"Error: {e}")
            print(f"Searched in: {sys.path}")
            sys.exit(1)
    
          # Initialize matcher
          matcher = CVJobMatcher()
    
          # Load CV
          cv_file = Path('${{ github.event.inputs.cv_file }}' or 'cv/my_cv.txt')
          if not cv_file.exists():
            # Try to find any CV file
            cv_files = list(Path('cv').glob('*.txt')) + list(Path('cv').glob('*.md'))
          if cv_files:
            cv_file = cv_files[0]
          else:
            print("‚ùå No CV file found in cv/ directory")
            sys.exit(1)
    
          cv_text = cv_file.read_text()
          print(f"‚úì Loaded CV from {cv_file}")
    
          # Load discovered jobs
          input_file = Path('discovered_jobs.json')
          if not input_file.exists():
            print(f"‚ùå Input file {input_file} not found")
            sys.exit(1)
    
          import json
          with open(input_file, 'r') as f:
             jobs_data = json.load(f)
    
          print(f"‚úì Loaded {len(jobs_data)} jobs from {input_file}")
    
          # Process each job
          threshold = float('${{ github.event.inputs.match_threshold }}' or '75')
          matched_jobs = []
    
          print(f"\nüîç Analyzing jobs with threshold {threshold}%...\n")
    
          for idx, job in enumerate(jobs_data, 1):
             job_text = f"{job.get('title', '')} {job.get('description', '')} {job.get('requirements', '')}"
        
          # Run analysis
          result = matcher.analyze_job(cv_text, job_text)
        
          # Add match score to job data
          job['match_score'] = result.get('overall_score', 0)
          job['match_details'] = result
        
          # Filter by threshold
          if job['match_score'] >= threshold:
            matched_jobs.append(job)
            print(f"‚úÖ Job {idx}/{len(jobs_data)}: {job.get('title')} - {job['match_score']}%")
          else:
            print(f"‚è≠Ô∏è  Job {idx}/{len(jobs_data)}: {job.get('title')} - {job['match_score']}% (below threshold)")
    
          # Save matched jobs
          output_file = Path('matched_jobs.json')
          with open(output_file, 'w') as f:
            json.dump(matched_jobs, f, indent=2)
    
          print(f"\n‚úÖ {len(matched_jobs)} jobs matched (threshold: {threshold}%)")
          print(f"‚úÖ Results saved to {output_file}")
    
          # Generate summary report
          report_lines = [
          "="*80,
          "JOB MATCHING SUMMARY",
          "="*80,
          f"Total jobs analyzed: {len(jobs_data)}",
          f"Jobs matched: {len(matched_jobs)}",
          f"Match threshold: {threshold}%",
          f"Match rate: {len(matched_jobs)/len(jobs_data)*100:.1f}%",
          "",
          "TOP MATCHES:",
          "-"*80
          ]
    
          # Sort by match score and show top 10
          top_matches = sorted(matched_jobs, key=lambda x: x['match_score'], reverse=True)[:10]
          for job in top_matches:
            report_lines.append(f"{job['match_score']}% - {job.get('title')} at {job.get('company', 'Unknown')}")
            report_lines.append(f"       {job.get('location', 'Location unknown')}")
            report_lines.append("")
    
          report = "\n".join(report_lines)
    
          # Save report
          report_file = Path('reports') / f'matching_summary_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt'
          report_file.write_text(report)
    
          print("\n" + report)
          PYTHON_SCRIPT
      
      - name: Categorize Matches by Platform
        run: |
          python3 << 'EOF'
          import json
          from collections import defaultdict
          
          with open('matched_jobs.json', 'r') as f:
              matches = json.load(f)
          
          by_platform = defaultdict(list)
          for job in matches:
              platform = job.get('source') or job.get('platform', 'unknown')
              by_platform[platform].append(job)
          
          print(f"\nüìä Matches by Platform:")
          for platform, jobs in sorted(by_platform.items()):
              print(f"   {platform}: {len(jobs)} jobs")
              avg_score = sum(j.get('match_score', 0) for j in jobs) / len(jobs) if jobs else 0
              print(f"      Average score: {avg_score:.1f}")
          EOF
      
      - name: Optimize Resumes for Top Matches
        run: |
          echo "üìù Optimizing resumes..."
          python ${{ env.SCRIPTS_DIR }}/Resume-Cover-Letter_Optimizer.py \
            --jobs matched_jobs.json \
            --resume-dir documents/resumes \
            --output resume_versions.json
      
      - name: Update Job Search Tracker
        run: |
          python ${{ env.SCRIPTS_DIR }}/job_search_tracker.py --update --source matched_jobs.json
      
      - name: Upload Matched Jobs
        uses: actions/upload-artifact@v4
        with:
          name: matched-jobs
          path: |
            matched_jobs.json
            resume_versions.json
            discovered_jobs.json
          retention-days: 30
          
  # ========================================
  # STAGE 4: FINAL STATUS
  # ========================================
  finalize:
    name: Pipeline Completion
    runs-on: ubuntu-latest
    needs: [initialize, discover-jobs, match-and-score, extract-keywords]
    if: always()
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Add Scripts to Path
        run: |
          echo "PYTHONPATH=$GITHUB_WORKSPACE/${{ env.SCRIPTS_DIR }}:$PYTHONPATH" >> $GITHUB_ENV
      
      - name: Check Pipeline Status
        run: |
          echo "üèÅ Job Search Manager Center - Multi-Platform Pipeline Complete"
          echo "Run ID: ${{ needs.initialize.outputs.run_id }}"
          echo "Mode: ${{ needs.initialize.outputs.mode }}"
          echo "Platforms: ${{ needs.initialize.outputs.platforms }}"
          echo "Timestamp: ${{ needs.initialize.outputs.timestamp }}"
          echo ""
          echo "Job Status:"
          echo "  Initialize: ${{ needs.initialize.result }}"
          echo "  Discovery: ${{ needs.discover-jobs.result }}"
          echo "  Keyword Extraction: ${{ needs.extract-keywords.result }}"
          echo "  Matching: ${{ needs.match-and-score.result }}"
      
      - name: Download All Artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: '*-jobs*'
          merge-multiple: true
      
      - name: Generate Platform Performance Report
        run: |
          python3 << 'EOF'
          import json
          from pathlib import Path
          from collections import defaultdict
          
          stats = defaultdict(dict)
          
          # Collect stats from all platforms
          for job_file in Path('.').glob('*_jobs.json'):
              try:
                  platform = job_file.stem.replace('_jobs', '')
                  with open(job_file, 'r') as f:
                      jobs = json.load(f)
                  
                  stats[platform]['total_jobs'] = len(jobs)
                  stats[platform]['unique_companies'] = len(set(j.get('company', '') for j in jobs))
                  
                  # Count jobs with salary info
                  with_salary = sum(1 for j in jobs if j.get('salary'))
                  stats[platform]['salary_info_rate'] = f"{with_salary/len(jobs)*100:.1f}%" if jobs else "0%"
              except:
                  pass
          
          print("\n" + "="*60)
          print("üìä PLATFORM PERFORMANCE SUMMARY")
          print("="*60)
          
          for platform, data in sorted(stats.items()):
              print(f"\n{platform.upper()}:")
              for key, value in data.items():
                  print(f"  {key.replace('_', ' ').title()}: {value}")
          
          print("\n" + "="*60)
          EOF
      
      - name: Update Run Log
        run: |
          python3 << 'EOF'
          import json
          from datetime import datetime
          from pathlib import Path
          
          log_file = Path('logs/pipeline_runs.json')
          log_file.parent.mkdir(exist_ok=True)
          
          try:
              with open(log_file, 'r') as f:
                  runs = json.load(f)
          except:
              runs = []
          
          run_record = {
              'run_id': '${{ needs.initialize.outputs.run_id }}',
              'timestamp': '${{ needs.initialize.outputs.timestamp }}',
              'mode': '${{ needs.initialize.outputs.mode }}',
              'platforms': '${{ needs.initialize.outputs.platforms }}',
              'status': {
                  'initialize': '${{ needs.initialize.result }}',
                  'discovery': '${{ needs.discover-jobs.result }}',
                  'keyword_extraction': '${{ needs.extract-keywords.result }}',
                  'matching': '${{ needs.match-and-score.result }}'
              }
          }
          
          runs.append(run_record)
          
          with open(log_file, 'w') as f:
              json.dump(runs[-100:], f, indent=2)
          
          print("‚úÖ Pipeline run logged")
          EOF
      
      - name: Commit Updated Logs
        run: |
          git config user.name "Job Search Bot"
          git config user.email "bot@jobsearch.local"
          git add logs/
          git commit -m "üìä Pipeline run log - ${{ needs.initialize.outputs.run_id }}" || echo "No log changes"
          git push || echo "Push failed"
      
      - name: Generate Summary Report
        run: |
          python3 << 'EOF'
          import json
          from pathlib import Path
          from datetime import datetime
          
          print("\n" + "="*70)
          print("üìã PIPELINE EXECUTION SUMMARY")
          print("="*70)
          print(f"\nRun ID: ${{ needs.initialize.outputs.run_id }}")
          print(f"Timestamp: ${{ needs.initialize.outputs.timestamp }}")
          print(f"Mode: ${{ needs.initialize.outputs.mode }}")
          print(f"Platforms: ${{ needs.initialize.outputs.platforms }}")
          
          # Count total jobs discovered
          total_jobs = 0
          for job_file in Path('.').glob('*_jobs.json'):
              try:
                  with open(job_file, 'r') as f:
                      jobs = json.load(f)
                  total_jobs += len(jobs)
              except:
                  pass
          
          print(f"\nüìä Results:")
          print(f"  Total jobs discovered: {total_jobs}")
          
          # Check for matched jobs
          if Path('matched_jobs.json').exists():
              with open('matched_jobs.json', 'r') as f:
                  matched = json.load(f)
              print(f"  Jobs matched: {len(matched)}")
              
              if matched:
                  avg_score = sum(j.get('match_score', 0) for j in matched) / len(matched)
                  print(f"  Average match score: {avg_score:.1f}")
          
          print("\n" + "="*70)
          EOF
      
      - name: Upload Final Report
        uses: actions/upload-artifact@v4
        with:
          name: pipeline-summary-${{ needs.initialize.outputs.run_id }}
          path: |
            logs/
            *.json
          retention-days: 90
